{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gmihaila/stock_risk_prediction/blob/master/notebooks/train_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulgiixgIW6Ai"
   },
   "source": [
    "# Info\n",
    "\n",
    "* Main Dataset: [S&P 500 stock data](https://www.kaggle.com/camnugent/sandp500)\n",
    "\n",
    "* Download detailes for each company: [S&P 500 Companies with Financial Information](https://datahub.io/core/s-and-p-500-companies-financials#resource-s-and-p-500-companies-financials_zip)\n",
    "\n",
    "Stock prices are flutuated in every day. So, in each day, put those stocks in order of price change to one sentence. Then, with certain window size, each stock will show up with highly related stock frequently, because they tend to move their prices together. Source: [stock2vec repo](https://github.com/kh-kim/stock2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2+cu113\n",
      "True\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(\"Num GPUs Available: \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n",
      "4.1.2\n",
      "2.8.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# it works well in python 3.8, gensim 4.1 (for word2vec), and tensorflow 2.8 (for Elmo)\n",
    "import gensim\n",
    "import sys\n",
    "print(sys.version)\n",
    "print(gensim.__version__)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5UP4hpKYEwF"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1646257257806,
     "user": {
      "displayName": "Ziruo Yi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW2AMS70gbo0eFVbfW3W0d6jTYfiRfVNU-1SKrsQ=s64",
      "userId": "13885267170196786700"
     },
     "user_tz": 360
    },
    "id": "EsbmtkAXXTS3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b3jKgTE-CA_"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1646257260476,
     "user": {
      "displayName": "Ziruo Yi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW2AMS70gbo0eFVbfW3W0d6jTYfiRfVNU-1SKrsQ=s64",
      "userId": "13885267170196786700"
     },
     "user_tz": 360
    },
    "id": "jlRD9lrw-DRm"
   },
   "outputs": [],
   "source": [
    "def sort_dict(mydict, reversed=False):\n",
    "  return sorted(mydict.items(), key=operator.itemgetter(1), reverse=reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rl07Ke1pYHqb"
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1646257262297,
     "user": {
      "displayName": "Ziruo Yi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW2AMS70gbo0eFVbfW3W0d6jTYfiRfVNU-1SKrsQ=s64",
      "userId": "13885267170196786700"
     },
     "user_tz": 360
    },
    "id": "cL2ITYKD6osW",
    "outputId": "da32879e-461e-4e3f-edca-f99539eb6c9e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Companies Details\n",
      "  Symbol                Name  Sector\n",
      "0    AAP  Advance Auto Parts       0\n",
      "1   AMZN      Amazon.com Inc       0\n",
      "2   APTV           Aptiv Plc       0\n",
      "3    AZO        AutoZone Inc       0\n",
      "4    BBY   Best Buy Co. Inc.       0\n",
      "\n",
      "Companies Stocks\n",
      "         date   open   high    low  close    volume Name\n",
      "0  2013-02-08  15.07  15.12  14.63  14.75   8407500  AAL\n",
      "1  2013-02-11  14.89  15.01  14.26  14.46   8882000  AAL\n",
      "2  2013-02-12  14.45  14.51  14.10  14.27   8126000  AAL\n",
      "3  2013-02-13  14.30  14.94  14.25  14.66  10259500  AAL\n",
      "4  2013-02-14  14.94  14.96  13.16  13.99  31879900  AAL\n"
     ]
    }
   ],
   "source": [
    "# Companies description\n",
    "desc_df = pd.read_csv('stocks_data//constituents - numerical.csv')\n",
    "print('\\nCompanies Details')\n",
    "print(desc_df.head())\n",
    "\n",
    "# stocks details\n",
    "stocks_df = pd.read_csv('stocks_data//all_stocks_5yr.csv')#, parse_dates=['date'])\n",
    "print('\\nCompanies Stocks')\n",
    "print(stocks_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUtMystc-rRd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 124,
     "status": "ok",
     "timestamp": 1646257266544,
     "user": {
      "displayName": "Ziruo Yi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW2AMS70gbo0eFVbfW3W0d6jTYfiRfVNU-1SKrsQ=s64",
      "userId": "13885267170196786700"
     },
     "user_tz": 360
    },
    "id": "QAuu7VFbzEb_"
   },
   "outputs": [],
   "source": [
    "# dicitonary for companies name and sector\n",
    "companies_names = {symbol:name for symbol, name in desc_df[['Symbol', 'Name']].values}\n",
    "companies_sector = {symbol:sector for symbol, sector in desc_df[['Symbol', 'Sector']].values}\n",
    "\n",
    "# get all companies symbols\n",
    "symbols = stocks_df['Name'].values\n",
    "dates = set(stocks_df['date'].values)\n",
    "dates = sorted(dates)\n",
    "\n",
    "# store each individual date and all its stocks\n",
    "dates_dictionary = {date:{} for date in dates}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1646257268129,
     "user": {
      "displayName": "Ziruo Yi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW2AMS70gbo0eFVbfW3W0d6jTYfiRfVNU-1SKrsQ=s64",
      "userId": "13885267170196786700"
     },
     "user_tz": 360
    },
    "id": "2MXCHuRMzEcA",
    "outputId": "adf88e8e-8aca-43ac-c559-53c5a65a264e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AAP', 0), ('AMZN', 0), ('APTV', 0), ('AZO', 0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just take a look companies_sector in list\n",
    "list(companies_sector.items())[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w10zJhiK_dyn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data for Word Embeddings\n",
    "\n",
    "For each date in out dataset we rearrange each company in ascending order based on the **change in price**.\n",
    "\n",
    "Formula for **change in price** [source](https://pocketsense.com/calculate-market-price-change-common-stock-4829.html):\n",
    "* (closing_price - opening_price) / opening_price\n",
    "\n",
    "We can change the formula to use highest price and lowest price. This is something we will test out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2004,
     "status": "ok",
     "timestamp": 1646257277963,
     "user": {
      "displayName": "Ziruo Yi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW2AMS70gbo0eFVbfW3W0d6jTYfiRfVNU-1SKrsQ=s64",
      "userId": "13885267170196786700"
     },
     "user_tz": 360
    },
    "id": "p4ybIa6yW7kP",
    "outputId": "4b98e7ca-eb15-420c-9251-5ef6cb2b49c2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate price change for each stock and sort them in each day\n",
    "for date, symbol, op, cl, in stocks_df[['date', 'Name', 'open', 'close']].values:\n",
    "  # CHANGE IN PRICE: (closing_price - opening_price) / opening_price\n",
    "  dates_dictionary[date][symbol] = (cl - op)/op\n",
    "# sort each day reverse order\n",
    "dates_dictionary = {date:sort_dict(dates_dictionary[date]) for date in dates}\n",
    "\n",
    "stocks_w2v_data = [[value[0] for value in dates_dictionary[date]] for date in dates]\n",
    "\n",
    "# print sample\n",
    "# print(stocks_w2v_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VaGbCgYATEf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (505, 10)\n"
     ]
    }
   ],
   "source": [
    "word_model = Word2Vec(stocks_w2v_data, vector_size=10, min_count=1, \n",
    "                                    window=5)\n",
    "X = word_model.wv.vectors\n",
    "vocab_size, emdedding_size = X.shape\n",
    "print('Result embedding shape:', X.shape)\n",
    "\n",
    "# print('Checking similar words:')\n",
    "# for word in ['model', 'network', 'train', 'learn']:\n",
    "#     most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.wv.most_similar(word)[:8])\n",
    "# print('  %s -> %s' % (word, most_similar))\n",
    "\n",
    "# def word2idx(word):\n",
    "    \n",
    "#     return word_model.wv.vocab[word].index\n",
    "# def idx2word(idx):\n",
    "#     return word_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(word_model.wv.key_to_index.keys())\n",
    "Y = []\n",
    "for word in words:\n",
    "    Y.append(companies_sector[word])\n",
    "    \n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "Y_oh = enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, \n",
    "                    weights=[X]))\n",
    "model.add(LSTM(units=emdedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45/45 [==============================] - 8s 17ms/step - loss: 6.0783 - accuracy: 0.1756\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 5.3421 - accuracy: 0.2323\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 4.1161 - accuracy: 0.2323\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 3.2107 - accuracy: 0.2323\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.7271 - accuracy: 0.2323\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.4871 - accuracy: 0.2323\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 2.3588 - accuracy: 0.2323\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 2.2874 - accuracy: 0.2323\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 2.2433 - accuracy: 0.2323\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.2117 - accuracy: 0.2323\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.1892 - accuracy: 0.2323\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.1741 - accuracy: 0.2323\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 2.1618 - accuracy: 0.2323\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.1526 - accuracy: 0.2323\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.1432 - accuracy: 0.2323\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 2.1382 - accuracy: 0.2323\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 2.1324 - accuracy: 0.2323\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 2.1270 - accuracy: 0.2323\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 2.1254 - accuracy: 0.2323\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 2.1240 - accuracy: 0.2323\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=8, epochs=20)\n",
    "preds1 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505 1 (101,)\n",
      "[0 0 3 0 0 4 4 7 2 3 2 2 9 1 4 5 0 4 1 1 0 5 5 0 5 6 7 2 4 0 0 4 2 4 0 0 3\n",
      " 3 0 5 3 2 3 6 5 0 3 4 5 0 3 6 1 3 2 0 0 3 2 5 2 2 0 7 5 5 4 6 2 9 5 3 2 0\n",
      " 2 0 9 0 6 3 4 7 2 5 0 7 1 9 4 4 9 5 7 3 6 2 5 3 3 3 4]\n"
     ]
    }
   ],
   "source": [
    "featureNumber = 15\n",
    "labels_str = ['Industrials' ,'Health Care' ,'Information Technology' ,'Utilities','Financials','Materials', \n",
    "                     'Consumer Discretionary','Real Estate', 'Consumer Staples','Energy',\n",
    "                     'Telecommunication Services']\n",
    "\n",
    "labels = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "counter = []\n",
    "classifier1_array = []\n",
    "\n",
    "# test accuracy at various number of dimensions\n",
    "for j in range(1,21):\n",
    "    word_model = Word2Vec(stocks_w2v_data, min_count=1, vector_size=j,  window=5)\n",
    "    words = list(word_model.wv.key_to_index.keys())\n",
    "    \n",
    "    # X = model[model.wv.vocab] aims to get all vectors. now we can use model.wv.vectors instead\n",
    "    X = word_model.wv.vectors \n",
    "    Y = []\n",
    "    vocab_size, emdedding_size = X.shape\n",
    "    print(vocab_size, emdedding_size, Y_test.shape)\n",
    "    for word in words:\n",
    "        Y.append(companies_sector[word])\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # split data for cross validation\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    print(Y_test)\n",
    "    break\n",
    "    \n",
    "    \n",
    "#     # predict sectors using GaussianNB, SVM, DecisionTreeClassifier and RandomForestClassifier\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, \n",
    "#                         weights=[X]))\n",
    "#     model.add(LSTM(units=emdedding_size))\n",
    "#     model.add(Dense(units=vocab_size))\n",
    "#     model.add(Activation('softmax'))\n",
    "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=8, epochs=20)\n",
    "    preds1 = model.predict(X_test)\n",
    "# #     print((preds1))\n",
    "# #     break\n",
    "#     print('R2 Score: ', r2_score(Y_test, preds1))\n",
    "\n",
    "#     classifier1_array.append(r2_score(Y_test, preds1))    \n",
    "#     counter.append(j)\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# pyplot.plot(counter,classifier1_array)\n",
    "# pyplot.ylabel('Accuracy')\n",
    "# pyplot.xlabel('Dimensions')\n",
    "# gnb_patch=mpatches.Patch(color='blue', label='GaussianNB')\n",
    "# pyplot.legend(handles=[gnb_patch,svm_patch, dtc_patch, rfc_patch], loc='best')\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
